---
title: "INFSCI 2595 Fall 2021 Homework: 10"
subtitle: "Assigned November 12, 2021; Due: November 19, 2021"
author: "Yifei Tai"
date: "Submission time: November 19, 2021 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

Include the names of your collaborators here.  

## Overview

This homework assignment is focused on working with logistic regression, a type of generalized linear models. You will fit, make predictions with, and assess model performance. You will also derive the gradient vector of the log-posterior with respect to the unknown coefficients. This way you get first hand experience working with the derivatives, and thus the gradient of logistic regression.  

**IMPORTANT**: Problems 03 and 04 are associated with derivations. If you concerned about working through derivations of the gradient, start with the programming portions of the assignment for Problems 01, 02, and 05.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are allowed to add as many code chunks as you see fit to answer the questions.  

## Load packages

This assignment will use packages from the `tidyverse` suite.  

```{r, load_packages, eval=TRUE}
library(tidyverse)
library(coefplot)
```



This assignment also uses the `splines` and `MASS` packages. Both are installed with base `R` and so you do not need to download any additional packages to complete the assignment.  

## Problem 01

You defined log-posterior functions for linear models in previous assignments. You worked with simple linear relationships, interactions, polynomials, and more complex spline basis features. In lecture, we discussed how the linear model framework can be *generalized* to handle non-continuous binary outcomes. The likelihood changed from a Gaussian to a Binomial distribution and a non-linear **link** function is required. In this way, the linear model is applied to a *linear predictor* which "behaves" like the trend in an ordinary linear model. In this problem, you will define the log-posterior function for logistic regression. By doing so you will be able to directly contrast what you did to define the log-posterior function for the linear model in previous assignments.  

### 1a)

The complete probability model for logistic regression consists of the likelihood of the response $y_n$ given the event probability $\mu_n$, the inverse link function between the probability of the event, $\mu_n$, and the linear predictor, $\eta_n$, and the prior on all linear predictor model coefficients $\boldsymbol{\beta}$.  

As in lecture, you will assume that the $\boldsymbol{\beta}$-parameters are a-priori independent Gaussians with a shared prior mean $\mu_{\beta}$ and a shared prior standard deviation $\tau_{\beta}$.  

**Write out complete probability model for logistic regression. You must write out the $n$-th observation's linear predictor using the inner product of the $n$-th row of a design matrix $\mathbf{x}_{n,:}$ and the unknown $\boldsymbol{\beta}$-parameter column vector. You can assume that the number of unknown coefficients is equal to $D + 1$.**  

You are allowed to separate each equation into its own equation block.  

*HINT*: The "given" sign, the vertical line, $\mid$, is created by typing `\mid` in a latex math expression. The product symbol (the giant PI) is created with `prod_{}^{}`.  

#### SOLUTION

Add in your equations here. 
$$
p (\mathbf{y} \mid \boldsymbol{\mu}) =
\prod_{n=1}^{N} \left(Bernoulli \left(y_{n} \mid \mu_{n} \right) \right)
$$
$$
\mu_n = \mathbf{logit}^{-1} \left(\eta_n \right)
$$
$$
\eta_n = \mathbf{x}_{n,:}\boldsymbol{\beta}
$$
$$
p(\boldsymbol{\beta}) = 
\prod_{d=0}^{D} \left(normal \left(\boldsymbol{\beta}_d \mid \mu_{\beta}, \tau_{\beta} \right) \right)
$$
$$
p(\boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X}) =
\prod_{n=1}^{N} \left(Bernoulli \left(y_{n} \mid \mathbf{logit}^{-1} \left[\mathbf{x}_{n,:}\boldsymbol{\beta} \right] \right) \right) 
\prod_{d=0}^{D} \left(normal \left(\boldsymbol{\beta}_d \mid \mu_{\beta}, \tau_{\beta} \right) \right)
$$


### 1b)

The code chunk below loads in a data set consisting of two variables. A continuous input `x` and a binary outcome `y`. The binary outcome is encoded as 0 if the event does not occur and 1 if the event does occur. The `count()` function is used to count the number of observations associated with each binary class in the second code chunk. As shown below, the event occurs more frequently than the non-event, but the two classes are not overly imbalanced.  

```{r, read_glm_dataset, eval=TRUE}
train_data_url <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2021/main/HW/10/hw10_data_01.csv'
train_df <- readr::read_csv(train_data_url, col_names = TRUE)

train_df %>% glimpse()
```


```{r, check_outcome_count, eval=TRUE}
train_df %>% count(y)
```

You will fit three logistic regression models. The first will be a linear relationship for the linear predictor, the second will be a cubic polynomial relationship, and the third will be a 7 degree-of-freedom natural spline. The linear predictor for the linear relationship model is:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n
$$

The linear predictor for the cubic polynomial is:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n + \beta_2 x_{n}^2 + \beta_3 x_{n}^3
$$

Assuming the $j$-th spline feature is $\phi_j\left(x\right)$ the 7th degree of freedom natural spline can be written as a summation series:  

$$ 
\eta_n = \beta_0 + \sum_{j=1}^{J=7} \left( \phi_j \left(x_n\right) \right)
$$

You will be defining a log-posterior function in the style of those from previous assignments. However, before doing so, you will create lists of required information for each of the desired models. You must create the design matrices associated with the linear and cubic polynomials, and the 7 degree-of-freedom spline. You must assign those design matrices to the `Xmat_linear`, `Xmat_cubic`, and `Xmat_spline7` objects. You must then complete the lists `info_linear`, `info_cubic`, and `info_spline7` by setting the observed responses to the `yobs` variable, the design matrices to the `design_matrix` variable, and also set the $\boldsymbol{\beta}$ prior hyperparameters.  

**Create the design matrices for the 3 models and specify the lists of required information. Specify the prior mean to be 0 and the prior standard deviation to be 5.**  

#### SOLUTION

Create the design matrices.  

```{r, solution_01b_a, eval=TRUE}
Xmat_linear <- model.matrix(y ~ x, data = train_df)

Xmat_cubic <- model.matrix(y ~ x + I(x^2) + I(x^3), data = train_df)

Xmat_spline7 <- model.matrix(y ~ splines::ns(x, 7), data = train_df)
```


Create the lists of required information.  

```{r, solution_01b_b, eval=TRUE}
info_linear <- list(
  yobs = train_df$y,
  design_matrix = Xmat_linear,
  mu_beta = 0,
  tau_beta = 5
)

info_cubic <- list(
  yobs = train_df$y,
  design_matrix = Xmat_cubic,
  mu_beta = 0,
  tau_beta = 5
)

info_spline7 <- list(
  yobs = train_df$y,
  design_matrix = Xmat_spline7,
  mu_beta = 0,
  tau_beta = 5
)
```

### 1c)

You will now define the log-posterior function for logistic regression, `logistic_logpost()`. The first argument to `logistic_logpost()` is the vector of unknowns and the second argument is the list of required information. You will assume that the variables within the `my_info` list are those contained in the `info_linear` list you defined previously.  

**Complete the code chunk to define the `logistic_logpost()` function. The comments describe what you need to fill in. Do you need to separate out the $\boldsymbol{\beta}$-parameters from the vector of `unknowns`?**  

**After you complete `logistic_logpost()`, test it by setting the `unknowns` vector to be a vector of -1's and then 2's for the linear relationship case. If you have successfully programmed the function you should get `-164.6232` and `-130.1428` for the -1 test case and +2 test case, respectively.**  

#### SOLUTION

Do you need to separate the $\boldsymbol{\beta}$-parameters from the `unknowns` vector?  

We don't need to separate.

```{r, solution_01c, eval=TRUE}
logistic_logpost <- function(unknowns, my_info)
{
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- X %*% unknowns
  
  # calculate the event probability
  mu <- boot::inv.logit(eta)
  
  # evaluate the log-likelihood
  log_lik <- sum(dbinom(x = my_info$yobs,
                        size = 1,
                        prob = mu,
                        log = TRUE))
  
  # evaluate the log-prior
  log_prior <- sum(dnorm(x = unknowns,
                         mean = my_info$mu_beta,
                         sd = my_info$tau_beta,
                         log = TRUE))
  
  # sum together
  log_lik + log_prior
}
```

Test out your function using the linear relationship information and setting the unknowns to a vector of -1's.  

```{r, solution_01c_b, eval=TRUE}
###
logistic_logpost(c(-1, -1), info_linear)
```

Test out your function using the linear relationship information and setting the unknowns to a vector of 2's.  

```{r, solution_01c_c, eval=TRUE}
###
logistic_logpost(c(2, 2), info_linear)
```

### 1d)

The `my_laplace()` function is provided to you in the code chunk below.  

```{r, define_my_laplace_func, eval=TRUE}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 1001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```


You will use `my_laplace()` to execute the Laplace Approximation for the three models, using initial guess values of zero. After fitting the models, calculate the middle 95% uncertainty intervals for the linear and cubic model parameters.  

**Perform the Laplace Approximation for all three models. The linear relationship should be assigned to the `laplace_linear` object, the cubic relationship to the `laplace_cubic`, and the 7 degree-of-freedom spline should be assigned to the `laplace_spline7` object. Should you be concerned that the initial guess will impact the results?**  

**After fitting, calculate the middle 95% uncertainty interval on the $\boldsymbol{\beta}$ parameters for the linear and cubic models. Which parameters contain zero in the middle 95% uncertainty interval?**  

#### SOLUTION

What do you think?  

```{r, solution_01d_a, eval=TRUE}
laplace_linear <- my_laplace(rep(0, ncol(Xmat_linear)), logistic_logpost, info_linear)

laplace_cubic <- my_laplace(rep(0, ncol(Xmat_cubic)), logistic_logpost, info_cubic)

laplace_spline7 <- my_laplace(rep(0, ncol(Xmat_spline7)), logistic_logpost, info_spline7)
```

Calculate the 95% uncertainty intervals on the linear relationship model.  

```{r, solution_01d_b, eval=TRUE}
###
upper_line <- laplace_linear$mode + 2 * sqrt(diag(laplace_linear$var_matrix))
lower_line <- laplace_linear$mode- 2 * sqrt(diag(laplace_linear$var_matrix))
upper_line
lower_line
```

Calculate the 95% uncertainty intervals on the cubic relationship model.  

```{r, solution_01d_c}
### 
#class(laplace_linear)
upper_cubic <- laplace_cubic$mode+2*sqrt(diag(laplace_cubic$var_matrix))
lower_cubic <- laplace_cubic$mode-2*sqrt(diag(laplace_cubic$var_matrix))
upper_cubic
lower_cubic

```

We don't need to consider about initial guess. Beta0 contains the zero in cubic model.

### 1e)

Let's compare the performance of the models using the Evidence-based assessment.  

**You must calculate the posterior model weight associated with each model. Based on your results, which model do you think is better?**  

#### SOLUTION

```{r, solution_01e, eval=TRUE}
###
weight_linear <- exp(laplace_linear$log_evidence) / (exp(laplace_linear$log_evidence) + exp(laplace_cubic$log_evidence) + exp(laplace_spline7$log_evidence))
weight_cubic <- exp(laplace_cubic$log_evidence) / (exp(laplace_linear$log_evidence) + exp(laplace_cubic$log_evidence) + exp(laplace_spline7$log_evidence))
weight_splines <- exp(laplace_spline7$log_evidence) / (exp(laplace_linear$log_evidence) + exp(laplace_cubic$log_evidence) + exp(laplace_spline7$log_evidence))
weight_linear
weight_cubic
weight_splines
```
Cubic model is the best model.


## Problem 02

In Problem 01, you compared the linear and cubic relationships based on the Evidence. Your assessment considered how well the model "fit" the data via the likelihood, based on the constraints imposed by the prior. The likelihood examines how likely the binary outcome is given the event probability. Thus, the Evidence is considering if the observations are consistent with the modeled event probability. In this problem, you will consider point-wise error metrics by calculating the confusion matrix associated with the training set. Confusion matrices are useful because the accuracy and errors are in the same "units" of the data.  

However, the logistic regression model predicts the event probability via the log-odds ratio. In order to move from the probability to the binary outcome a decision must be made. As discussed during the Applied Machine Learning portion of the course, the decision consists of comparing the predicted probability to a threshold value. If the predicted probability is greater than the threshold, classify the outcome as the event. Otherwise, classify the outcome as the non-event.  

In order to classify the training points, you must make posterior predictions with the logistic regression models you fit in Problem 01.

### 2a)

Although you were able to apply the `my_laplace()` function to both the regression and logistic regression settings, you cannot directly apply the `generate_lm_post_samples()` function from your previous assignments. You will therefore adapt `generate_lm_post_samples()` and define `generate_glm_post_samples()`. The code chunk below starts the function for you and uses just two input arguments, `mvn_result` and `num_samples`. You must complete the function.  

**Why can you not directly use the `generate_lm_post_samples()` function? Since the `length_beta` argument is NOT provided to `generate_glm_post_samples()`, how can you determine the number of $\boldsymbol{\beta}$-parameters? Complete the code chunk below by first assigning the number of $\boldsymbol{\beta}$-parameters to the `length_beta` variable. Then generate the random samples from the MVN distribution. You do not have to name the variables, you only need to call the correct random number generator.**  

#### SOLUTION

We cannot directly use the `generate_lm_post_samples()` because it includes the back-transformation from $\varphi$ to $\sigma$. The logistic regression model does **NOT** include $\sigma$. And so we should not use the function we used in the previous assignments. Since the only unknowns are the $\boldsymbol{\beta}$-parameters, we can determine `length_beta` by just using the length of the posterior mode vector.  

```{r, solution_02a, eval=TRUE}
generate_glm_post_samples <- function(mvn_result, num_samples)
{
  # specify the number of unknown beta parameters
  length_beta <- length(mvn_result$mode)
  
  # generate the random samples
  beta_samples <- MASS::mvrnorm(n = num_samples,
                                mu = mvn_result$mode,
                                Sigma = mvn_result$var_matrix)

  # change the data type and name
  beta_samples %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}
```


### 2b)

You will now define a function which calculates the posterior prediction samples on the linear predictor and the event probability. The function, `post_logistic_pred_samples()` is started for you in the code chunk below. It consists of two input arguments `Xnew` and `Bmat`. `Xnew` is a test design matrix where rows correspond to prediction points. The matrix `Bmat` stores the posterior samples on the $\boldsymbol{\beta}$-parameters, where each row is a posterior sample and each column is a parameter.  

**Complete the code chunk below by using matrix math to calculate the linear predictor at every posterior sample. Then, calculate the event probability for every posterior sample.**  

The `eta_mat` and `mu_mat` matrices are returned within a list, similar to how the `Umat` and `Ymat` matrices were returned for the regression problems.  

*HINT*: The `boot::inv.logit()` can take a matrix as an input. When it does, it returns a matrix as a result.  

#### SOLUTION

```{r, solution_02b, eval=TRUE}
post_logistic_pred_samples <- function(Xnew, Bmat)
{
  # calculate the linear predictor at all prediction points and posterior samples
  eta_mat <- Xnew %*% t(Bmat)
  
  # calculate the event probability
  mu_mat <- boot::inv.logit(eta_mat)
  
  # book keeping
  list(eta_mat = eta_mat, mu_mat = mu_mat)
}
```


### 2c)

The code chunk below defines a function `summarize_logistic_pred_from_laplace()` which manages the actions necessary to summarize posterior predictions of the event probability. The first argument, `mvn_result`, is the Laplace Approximation object. The second object is the test design matrix, `Xtest`, and the third argument, `num_samples`, is the number of posterior samples to make. You must follow the comments within the function in order to generate posterior prediction samples of the linear predictor and the event probability, and then to summarize the posterior predictions of the event probability.  

The result from `summarize_logistic_pred_from_laplace()` summarizes the posterior predicted event probability with the posterior mean, as well as the 5th and 95th quantiles. If you have completed the `post_logistic_pred_samples()` function correctly, the dimensions of the `mu_mat` matrix should be consistent with those from the `Umat` matrix from the regression problems.  

The posterior summary statistics summarize over all posterior samples. You must therefore choose between `colMeans()` and `rowMeans()` as to how to calculate the posterior mean event probability for each prediction point. The posterior quantiles are calculated for you.  

**Follow the comments in the code chunk below to complete the definition of the `summarize_logistic_pred_from_laplace()` function. You must generate posterior samples, make posterior predictions, and then summarize the posterior predictions of the event probability.**  

*HINT*: The result from `post_logistic_pred_samples()` is a list.

#### SOLUTION

```{r, solution_02, eval=TRUE}
summarize_logistic_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
  # generate posterior samples of the beta parameters
  betas <- generate_glm_post_samples(mvn_result, num_samples)
  
  # data type conversion
  betas <- as.matrix(betas)
  
  # make posterior predictions on the test set
  pred_test <- post_logistic_pred_samples(Xtest, betas)
  
  # calculate summary statistics on the posterior predicted probability
  # summarize over the posterior samples
  
  # posterior mean, should you summarize along rows (rowMeans) or 
  # summarize down columns (colMeans) ???
  mu_avg <- rowMeans(pred_test$mu_mat)
  
  # posterior quantiles
  mu_q05 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
  mu_q95 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)
  
  # book keeping
  tibble::tibble(
    mu_avg = mu_avg,
    mu_q05 = mu_q05,
    mu_q95 = mu_q95
  ) %>% 
    tibble::rowid_to_column("pred_id")
}
```


### 2d)

Summarize the posterior predicted event probability associated with the training set all three models. After making the predictions, a code chunk is provided for you which generates a figure showing how the posterior predicted probability summaries compare with the observed binary outcomes. Which of the three models appear to better capture the trends in the binary outcome?  

**Call `summarize_logistic_pred_from_laplace()` for the all three models on the training set. Specify the number of posterior samples to be 2500. Print the dimensions of the resulting objects to the screen. How many rows are in each data set?**  

**The third code chunk below uses the prediction summaries to visualize the posterior predicted event probability on the training set. Which relationship seems more in line with the observations?**  

#### SOLUTION

THe prediction summarizes should be executed in the code chunk below.  

```{r, solution_02_a, eval=TRUE}
set.seed(8123) 

post_pred_summary_linear <- summarize_logistic_pred_from_laplace(laplace_linear, Xmat_linear, 2500)

post_pred_summary_cubic <- summarize_logistic_pred_from_laplace(laplace_cubic, Xmat_cubic, 2500)

post_pred_summary_spline7 <- summarize_logistic_pred_from_laplace(laplace_spline7, Xmat_spline7, 2500)
post_pred_summary_linear
```

Print the dimensions of the objects to the screen.  

```{r, solution_02_b, eval=TRUE}
###
dim(post_pred_summary_linear)
dim(post_pred_summary_cubic)
dim(post_pred_summary_spline7)
```

The figure below is created for you.  

```{r, solutioN_02_c, eval=TRUE}
post_pred_summary_linear %>% 
  mutate(type = "linear relationship") %>% 
  bind_rows(post_pred_summary_cubic %>% 
              mutate(type = "cubic relationship")) %>% 
  bind_rows(post_pred_summary_spline7 %>% 
              mutate(type = "7 dof spline")) %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id") %>% 
  ggplot(mapping = aes(x = x)) +
  geom_ribbon(mapping = aes(ymin = mu_q05,
                            ymax = mu_q95,
                            group = type),
              fill = "steelblue", alpha = 0.5) +
  geom_line(mapping = aes(y = mu_avg,
                          group = type),
            color = "navyblue", size = 1.15) +
  geom_point(mapping = aes(y = y),
             size = 2.5, alpha = 0.2) +
  facet_grid( . ~ type) +
  labs(y = "y or event probability") +
  theme_bw()
```
1. There are 125 rows.
2. Cubic model is more in line with observation. Because the event probability is always change as the numbers of observation. The curve rises as the more event observed, and decreases as more non-event observed.

### 2e)

You will now consider classifying the predictions based upon a threshold value of 0.5. You will compare that threshold value to the posterior predicted event probabilities associated with the training set. Although the Bayesian model provides a full posterior predictive distribution, you will work just with the posterior mean value. Thus, you will create a single confusion matrix, rather than considering the uncertainty in the confusion matrix.  

Creating the confusion matrix is rather simple compared to some of the previous tasks in this assignment. The first step is to classify the prediction as event or non-event, which can be accomplished with an if-statement. The `ifelse()` function provides an "Excel-like" conditional statement, and is a simple way to perform the classification task. The syntax for `ifelse()` consists of three arguments, shown below:  

`ifelse(<conditional test>, <return if condition is TRUE>, <return if condition is FALSE>)`

The first argument is the conditional test you wish to apply. The second argument is what will be returned if the condition is true, and the third argument is what will be returned if the condition is false.  

You will use the `ifelse()` function to compare the posterior predicted mean event probability to the assumed threshold value of 0.5.  

**Pipe the `post_pred_summary_linear` object into a `mutate()` call and create a new variable `pred_class` which is the result of an `ifelse()` operation. For the conditional test, return a value of `1` if the posterior predicted mean event probability is greater than 0.5, and return `0` otherwise. Repeat the process for the `post_pred_summary_cubic` and `post_pred_summary_spline7` objects.**  

#### SOLUTION

```{r, solution_2e, eval=TRUE}
post_pred_summary_linear_class <- post_pred_summary_linear %>% mutate(pred_class = ifelse(mu_avg > 0.5, 1, 0)) 

post_pred_summary_cubic_class <- post_pred_summary_cubic %>% mutate(pred_class = ifelse(mu_avg > 0.5, 1, 0))

post_pred_summary_spline7_class <- post_pred_summary_spline7 %>% mutate(pred_class = ifelse(mu_avg > 0.5, 1, 0))

post_pred_summary_linear_class 

```

### 2f)

The code chunk below uses the `left_join()` function to merge the training data set, `train_df` with each of the posterior prediction summary objects. The results, `post_pred_summary_linear_class_b`, `post_pred_summary_cubic_class_b`, and `post_pred_summary_spline7_class_b` now have predicted classifications, `pred_class`, and observed outcomes `y`.  

```{r, merge_preds_and_train_obs, eval=TRUE}
post_pred_summary_linear_class_b <- post_pred_summary_linear_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")

post_pred_summary_cubic_class_b <- post_pred_summary_cubic_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")

post_pred_summary_spline7_class_b <- post_pred_summary_spline7_class %>% 
  left_join(train_df %>% tibble::rowid_to_column("pred_id"),
            by = "pred_id")
```


**Use the `count()` function to determine the confusion matrix associated with each relationship. How many true-positives, true-negatives, false-positives, and false-negatives does each relationship have?**  

#### SOLUTION

The confusion matrix for the linear relationship is shown below.  

```{r, solution_02f_a, eval=TRUE}
###
post_pred_summary_linear_class_b %>% count((pred_class == 1) & (y == 1), ((pred_class == 0) & (y == 0)), (pred_class == 1) & (y == 0), (pred_class == 0) & (y == 1))
```

The confusion matrix for the cubic relationship is shown below.  

```{r, solution_02f_b, eval=TRUE}
###
post_pred_summary_cubic_class_b %>% count((pred_class == 1) & (y == 1), ((pred_class == 0) & (y == 0)), (pred_class == 1) & (y == 0), (pred_class == 0) & (y == 1))
```

The confusion matrix for the 7 degree-of-freedom spline is shown below.  

```{r, solution_02f_c, eval=TRUE}
###
post_pred_summary_spline7_class_b %>% count((pred_class == 1) & (y == 1), ((pred_class == 0) & (y == 0)), (pred_class == 1) & (y == 0), (pred_class == 0) & (y == 1))
```

For the linear model:
tp =63  tn = 16 fp = 34 fn= 12

For the cubic model:
tp = 57 tn = 31 fp = 19 fn = 18

For the splines model:
tp = 62 tn = 29 fp = 21 fn = 13

## Problem 03

In lecture, we worked through the derivation of the gradient of the log-posterior associated with logistic regression assuming an infinitely diffuse prior on the $\boldsymbol{\beta}$ parameters. Thus, the gradient is equivalent to the gradient of the log-likelihood. You will now consider the case of an informative or regularizing prior. The un-normalized log-posterior on the unknown coefficients given the observations is therefore:  

$$ 
\log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \propto \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) + \log \left( p \left( \boldsymbol{\beta} \right) \right)
$$

In lecture, we worked through the derivations for the general case with $D$ inputs or predictors. For this problem however, you will consider a linear relationship associated with a single input, $x$. The $n$-th observation's linear predictor, $\eta_n$, is thus:  

$$ 
\eta_n = \beta_0 + \beta_1 x_n
$$

You will assume indepedent Gaussian priors on the intercept, $\beta_0$, and the slope, $\beta_1$, with prior mean $\mu_{\beta} = 0$ and prior standard deviation $\tau_{\beta} = b$. Thus, your final expressions should involve the $b$ hyperparameter.  

Your goal is to derive the gradient of the log-posterior with respect to the intercept, $\beta_0$, and the slope $\beta_1$. You will work through that derivation in steps.  

### 3a)

You will start by considering the derivatives of the log-prior, $\log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)$, with respect to the coefficients.  

**Derive the partial derivative of the log-prior with respect to the intercept $\beta_0$.**  

*HINT*: You can write the partial derivative as:  

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right)
$$

#### SOLUTION

Include as many equation blocks as you feel are necessary. 

$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
\frac{\partial}{\partial \beta_0} \left(\log \left(p \left(\beta_0 \mid 0, b \right)\right) +
\log \left( p \left(\beta_1 \mid 0, b \right) \right) \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
\frac{\partial}{\partial \beta_0} \left[\log \left[\frac{1}{\sqrt{2 \pi \tau_{\beta}^{2}}} \exp \left(- \frac{1}{2 \tau_{\beta}^{2}} \left(\beta_0 - \mu_{\beta} \right)^{2} \right) \right]\right] +
\frac{\partial}{\partial \beta_0} \left[
\log \left[\frac{1}{\sqrt{2 \pi \tau_{\beta}^{2}}} \exp \left(- \frac{1}{2 \tau_{\beta}^{2}} \left(\beta_1 - \mu_{\beta} \right)^{2} \right) \right] \right]
$$
Since the whole second portion is constant, so the result of derivative is 0.
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
\frac{\partial}{\partial \beta_0} \left(\log \left(\frac{1}{\sqrt{2 \pi \tau_{\beta}^{2}}} \right) + 
\left(- \frac{1}{2 \tau_{\beta}^{2}} \left(\beta_0 - \mu_{\beta} \right)^{2} \right) \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
\frac{\partial}{\partial \beta_0} \left(- \frac{1}{2 \tau_{\beta}^{2}} \left(\beta_0 - \mu_{\beta} \right)^{2} \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
- \frac{\beta_0 - \mu_{\beta}}{\tau_{\beta}^{2}}
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
- \frac{\beta_0}{b^{2}}
$$

### 3b)

**Derive the partial derivative of the log-prior with respect to the slope $\beta_1$.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary.  

Because the processing is just as same as 3a, so I simplify it.

$$
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
\frac{\partial}{\partial \beta_1} \left(\log \left(p \left(\beta_0 \mid 0, b \right)\right) +
\log \left( p \left(\beta_1 \mid 0, b \right) \right) \right)
$$
$$
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta} \mid 0, b \right) \right)  \right) = 
- \frac{\beta_1}{b^{2}}
$$

### 3c)

You will now consider the log-likelihood. You will start by considering the $n$-th observation's contribution to the log-likelihood, $L_n$.  

**Write out the expression for the $n$-th observation's log-likelihood in terms of the binary outcome $y_n$ and the event probability $\mu_n$.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary.  
$$
\log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) =
\sum_{n=1}^{N} \left(\log \left[p \left(y_n \mid \mu_n \right) \right] \right) 
$$
$$
\sum_{n=1}^{N} \left(\log \left[p \left(y_n \mid \mu_n \right) \right] \right) = 
\sum_{n=1}^{N} \left(y_n \log \left[\mu_n \right] + \left(1 - y_n \right) \log \left[1 - \mu_n \right] \right)
$$
So, the n-th observation is:
$$
L_n = y_n \log \left[\mu_n \right] + \left(1 - y_n \right) \log \left[1 - \mu_n \right] 
$$
### 3d)

**Write out the partial derivative of $L_n$ with respect to the intercept using the chain rule, considering the event probability, $\mu_n$, and the linear predictor, $\eta_n$.**  

#### SOLUTION

Due to the chain rule, we can get the following equation.
$$ 
\frac{\partial L_n}{\partial \beta_0} = \frac{\partial L_n}{\partial \mu_n} \frac{\partial \mu_n}{\partial \eta_n} \frac{\partial \eta_n}{\partial \beta_0}
$$

### 3e)

**Use the terms of the chain rule to derive the partial first derivative of $L_n$ with respect to the intercept $\beta_0$. You should show the derivatives associated with each term in the chain rule.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. 

First portion
$$
\frac{\partial L_n}{\partial \mu_n} = \frac{\partial}{\partial \mu_n} \left(y_n \log \left[\mu_n \right] + \left(1 - y_n \right) \log \left[1 - \mu_n \right] \right)
$$

$$
\frac{\partial L_n}{\partial \mu_n} = 
y_n \frac{\partial}{\partial \mu_n} \left(\log \left[\mu_n \right] \right) +
\left(1 - y_n \right) \frac{\partial}{\partial \mu_n} \left(\log \left[1 - \mu_n \right] \right)
$$

$$
\frac{\partial L_n}{\partial \mu_n} = \frac{y_n}{\mu_n} - \frac{1 - y_n}{1 - \mu_n}
$$
Second portion
$$
\frac{\partial \mu_n}{\partial \eta_n} = 
\frac{\partial}{\eta_n} \left(\textbf{logit}^{-1} \left(\eta_n \right) \right) =
\frac{\partial}{\eta_n} \left(\frac{\exp \left(\eta_n\right)}{1 + \exp \left(\eta_n\right) } \right)
$$

$$
\frac{\partial}{\eta_n} \left(\frac{\exp \left(\eta_n\right)}{1 + \exp \left(\eta_n\right) } \right) =
\frac{\exp\left(\eta_n \right) \left(1 + \exp \left(\eta_n\right) \right) - \left( \exp\left(\eta_n \right) \right)^{2} }{\left(1 + \exp \left(\eta_n\right) \right)^{2}}
$$

$$
\frac{\partial}{\eta_n} \left(\frac{\exp \left(\eta_n\right)}{1 + \exp \left(\eta_n\right) } \right) =
\mu_n \left(1 - \mu_n \right)
$$
Third portion
$$
\frac{\partial \eta_n}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \left(\beta_0 + \beta_1 x_n \right)
$$

$$
\frac{\partial \eta_n}{\partial \beta_0} = 1
$$
$$
\frac{\partial L_n}{\partial \beta_0} = \left( \frac{y_n}{\mu_n} - \frac{1 - y_n}{1 - \mu_n} \right) \times \left(\mu_n \left(1 - \mu_n \right) \right) \times 1
$$
$$
\frac{\partial L_n}{\partial \beta_0} = y_n - \mu_n
$$

### 3f)

**The partial derivative you determined in 3e) is associated with just a single observation. Write out the expression for the partial derivative of "complete log-likelihood" with respect to the intercept.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right) =
\frac{\partial}{\partial \beta_0} \left[
\sum_{n=1}^{N} \left(y_n \log \left[\mu_n \right] + \left(1 - y_n \right) \log \left[1 - \mu_n \right] \right) \right]
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right) =
\sum_{n=1}^{N} \left( \frac{\partial L_n}{\partial \beta_0} \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right) = 
\sum_{n=1}^{N} \left(\frac{\partial L_n}{\partial \mu_n} \frac{\partial \mu_n}{\partial \eta_n} \frac{\partial \eta_n}{\partial \beta_0}
 \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right)  \right) = 
\sum_{n=1}^{N} \left( y_n - \mu_n \right)
$$

### 3g)

**Combine your result in 3f) with the partial derivative of the log-prior with respect to the intercept from 3a) to write out the partial first derivative of the log-posterior with respect to the intercept.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right)
$$
$$
\frac{\partial}{\partial \beta_0} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right) = 
\sum_{n=1}^{N} \left( y_n - \mu_n \right) -
\frac{\beta_0}{b^{2}}
$$


## Problem 04

Problem 3) mostly focused on the derivatives with respect to the intercept, $\beta_0$. You will complete the derivation by considering the derivatives with respect to the slope, $\beta_1$.  

### 4a)

**Derive the partial first derivative of the log-likelihood with respect to the slope, $\beta_1$. If you feel can make use of any "patterns" from you derivations in Problem 3), state what those are.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) \right)
$$
$$
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \mathbf{y} \mid \boldsymbol{\beta}, \mathbf{X} \right) \right) \right) = 
\sum_{n=1}^{N} \left( \left( y_n - \mu_n \right) \times x_n \right)
$$

According to problem 3, we can get a kind of pattern that the partial first derivative of the log-likelihood will change as their chosen parameter changes which means the last portion in the chain rule expression. So if we do the derivative with respect to slope. We will get the two same portion and times a new portion: xn.

### 4b)

**Combine your result in 4a) with your result from 3b) to write out the partial first derivative of the log-posterior with respect to the slope.**  

#### SOLUTION

Include as many equation blocks as you feel are necessary. The equation block below shows how to write out the partial derivative as a hint.  

$$ 
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right)
$$
$$
\frac{\partial}{\partial \beta_1} \left( \log \left( p \left( \boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X} \right) \right) \right) =
\sum_{n=1}^{N} \left( \left( y_n - \mu_n \right) \times x_n \right) -
\frac{\beta_1}{b^{2}}
$$

### 4c)

**What impact does the prior have on the gradient relative to the gradient associated with the log-likelihood alone?**  

#### SOLUTION

What do you think?  

The prior gives the gradient one more portion. If the beta is positive, this portion will make gradient larger, and if it's negetive it will make gradient smaller.

## Problem 05

In lecture, we spent a considerable amount of time discussing a situation which challenges fitting logistic regression with maximum likelihood estimation. Now that you have fit several logistic regression models, and examined the influence of the prior on the gradient, let's get some practice working with this challenging situation.  

A small data set is read for you in the code chunk below, consisting of a single input, `x`, and a binary outcome `y`. As shown by the `glimpse()` this data set consists of just 5 observations.  

```{r, read_small_data_05}
small_data_url <- "https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2021/main/HW/10/hw10_data_small.csv"

train_small <- readr::read_csv( small_data_url )

train_small %>% glimpse()
```

### 5a)

**Plot the binary outcome, `y`, with respect to the continuous input, `x`. Include an additional layer to your graphic by adding in geom_vline() with the `xintercept` argument set to -0.6.**  

**Based on the figure, should we expect the logistic regression model to struggle with this small data set?**  

#### SOLUTION

```{r, solution_05a, eval=TRUE}
###
train_small %>% 
  ggplot(mapping = aes(x = x, y = y)) +
  geom_point(color = "blue") +
  geom_vline(xintercept = -0.6, color = "orange")
```

Yes, it will be. The seperate line can be infinite.

### 5b)

You will fit a Bayesian logistic regression model assuming a linear relationship between the input and the linear predictor:  

$$ 
\eta_n = \beta_0 + \beta_1 x_{n}
$$

You will fit two models using the linear relationship. The first will use an informative prior on the unknown $\boldsymbol{\beta}$ coefficients with a prior standard deviation of 2.5. The second model will use an very diffuse or *very weak* prior with a prior standard deviation of 50.  

Before fitting the models, you must create the lists of required information for this application. 

**Complete the code chunk below which creates the lists of required information for the informative and very weak priors using the small data set of just 5 observations.**  

**The informative prior should use the prior standard deviation of 2.5 and the very weak prior should use a prior standard deviation of 50. Both priors should use prior mean of 0.**  

#### SOLUTION

```{r, solution_05b, eval=TRUE}
Xmat <- model.matrix(y ~ x, data = train_small)
Xmat1 <- model.matrix(y ~ ., data = train_small)
info_small_inform <- list(
  yobs = train_small$y,
  design_matrix = Xmat,
  mu_beta = 0,
  tau_beta = 2.5
)

info_small_weak <- list(
  yobs = train_small$y,
  design_matrix = Xmat,
  mu_beta = 0,
  tau_beta = 50
)
```


### 5c)

You will now execute the Laplace Approximation to fit the Bayesian logistic regression models associated with the informative and weak priors.  

**Execute the laplace approximation for the two prior specifications. Assign the result associated with the informative prior to `small_laplace_from_inform` and the assign the result associated with the very weak prior to `small_laplace_from_weak`.**  

**How do the posterior modes compare between the two models? Does the informative prior cause the posterior mode to be different from the mode associated with the very weak prior?**  

#### SOLUTION

```{r, solution_05c_a, eval=TRUE}
small_laplace_from_inform <- my_laplace(rep(0, ncol(Xmat)), logistic_logpost, info_small_inform)
small_laplace_from_inform$mode
```


```{r, solution_05c_b, eval=TRUE}
small_laplace_from_weak <- my_laplace(rep(0, ncol(Xmat)), logistic_logpost, info_small_weak)
small_laplace_from_weak$mode
```

How do the posterior modes compare? Include text and code chunks to answer this question. 

According to the above results, we can get that the modes with informative prior are closer to zero than those with very weak prior. 

### 5d)

**Compare the posterior correlation between the intercept and slope associated with an informative prior to the posterior correlation associated with the very weak prior.**  

*HINT*: The Laplace Approximation gives you something that lets you easily calculate the correlation matrix...  

#### SOLUTION

Add as many code chunks as you feel are necessary to answer this question.  
```{r, eval=TRUE}
cor_info <- small_laplace_from_inform$var_matrix %>% cov2cor()
corrplot::corrplot(cor_info) 
```
```{r, eval=TRUE}
cor_weak <- small_laplace_from_weak$var_matrix %>% cov2cor()
corrplot::corrplot(cor_weak)
```

The posterior correlation associated with an informative prior is stronger than that associated with a weak prior.

### 5e)

**Discuss the differences between the posterior correlation between the coefficients when an informative prior is used compared to a very weak prior on this very small data set.** 

#### SOLUTION

What do you think? 

Because this is a small data set. So the effect from prior is stronger than data. When we use informative prior the posterior correlation between the coefficients is weaker than the posterior correlation associated with the weak prior. At the same time, weak prior will give more uncertainty in the beginning.

### 5f)

What do you think will happen if you would find the MLEs for the coefficients, and thus have no prior influence at all?  

**Use `glm()` to fit the logistic regression model via maximum likelihood estimation for the small data set. Assign the result to `mod_small_mle`.**

**Display the coefficient MLEs, what's going on with the maximum likelihood result for the small data set?**

#### SOLUTION
  
```{r, solution_05f, eval=TRUE}
###
mod_small_mle <- glm(y ~ x, family = "binomial", data = train_small)
coefplot(mod_small_mle)
#coef(mod_small_mle)
summary(mod_small_mle)
#mod_small_mle
```

Add as many code chunks as you feel are necessary and discussion text. 

According to the figure, we can see that the parameter is very weird. When the data set is too small, the logistic regression will overreact. Because the model want to predict the value 1 or 0. It's a sharp change or no change. This is why logistic regression requires more observations than the linear model to achieve the same posterior precision in the unknown coefficients.


