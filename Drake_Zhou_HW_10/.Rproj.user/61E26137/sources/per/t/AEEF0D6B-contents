---
title: "INFSCI 2595 Spring 2022 Homework: 10"
subtitle: "Assigned April 7, 2022; Due: April 14, 2022"
author: "Drake Zhou"
date: "Submission time: April 14, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

Include the names of your collaborators here.  

## Overview

This assignment focuses on the architecture of single hidden layer feedforward neural networks. You will practice calculating hidden units and neural network responses for a regression task. You will gain experience understanding the interaction between the hidden unit and output layer parameters. You will fit a regression neural network to data by minimizing the sum of squared errors (SSE), and tune the number of hidden units via a hold-out test set. Lastly, you will use `caret` to manage the resampling and tuning of a neural network for you.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are allowed to add as many code chunks as you see fit to answer the questions.  

## Load packages

This assignment will use packages from the `tidyverse` suite.  

```{r, load_packages}
library(tidyverse)
```

This assignment also uses the `scale_color_colorblind()` function from the `ggthemes` package. If you do not have `ggthemes` already installed please type `install.packages("ggthemes")` into the `R` console, or use the RStudio package installer GUI. You only need to run that command **ONCE**. Once the package is installed, you do **not** need to run the command again.  

## Problem 01

In lecture we discussed that neural networks are just matrix multiplications. Hidden units are essentially transformed linear models. The output layer is a linear basis function model with the hidden units acting as the basis functions. In this problem, you will work through the various matrix calculations. Neural networks have already been fit to a data set and the the neural network parameters (weights and biases) are provided to you.  

As practice, you will work with a noise-free toy problem. The response, $f$, is simply equal to $\mathrm{sin}\left(x\right)$. You will practice neural network calculations to approximate that functional relationship.  

The toy data set is loaded for you in the code chunk below and a glimpse is printed to screen. To distinguish between noisy observations (that we typically work with) the response is named `f` in the data set. The input is named `x` as in previous homework assignments.  

```{r, read_prob_01_data}
url_01 <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_01_train.csv'

prob_01_df <- readr::read_csv(url_01, col_names = TRUE)

prob_01_df %>% glimpse()
```

### 1a)

**Plot `f` with respect to `x` with `ggplot()`. Since the response is noise-free, use `geom_line()` and `geom_point()` geoms to make "connect the dots".**  

#### SOLUTION

```{r, solution_01a}
###
prob_01_df %>%
  ggplot()+
  geom_line(mapping = aes(x = x, y = f))+
  geom_point(mapping = aes(x = x, y = f))
```

### 1b)

A set of neural network parameters are downloaded in the code chunk below. The parameters are stored in a list consisting of two elements. The first element named, `beta_matrix` is a matrix containing the $\beta$-parameters associated with the hidden units. The second, `alpha_vector`, is a "regular vector" containing the output layer parameters. The list is printed to screen for you.  

```{r, read_prob_01_params_a}
url_load_dir <- 'https://github.com/jyurko/INFSCI_2595_Spring_2022/blob/main/HW/10'

url_load_01_2a <- paste(paste(url_load_dir, "hw_10_prob_01_params_2a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2a <- readr::read_rds( file = url_load_01_2a )

params_01_2a
```

The `$beta_matrix` is constructed such that each column corresponds to a separate *hidden unit* within the hidden layer. The particular neural network model associated with the `params_01_2a` parameters therefore has 2 hidden units.  

**Why does the `$beta_matrix` contain 2 rows and why does `$alpha_vector` consist of 3 elements? What does each $\beta$ parameter correspond to relative to the hidden units?**  

#### SOLUTION

What do you think?  

Because we have intercept included by applying inner product. Hence, the number of rows equals to the number of input plus one, which is 2 rows. What's more, each column represent a hidden unit. 

3 elements in alpha_vector compose by 2 hidden unit weight and 1 bias.

### 1c)

The hidden units consist of two calculations. The first calculates the "linear predictors" based on the inputs and hidden unit parameters. The second is a non-linear transformation of the "linear predictors". We discussed in lecture that there are many possible non-linear transformation functions to use, but the logistic function is a popular choice. Assume the inputs are stored in a design matrix $\mathbf{X}$, which includes a column of 1s, and the hidden unit parameters are stored in a matrix $\mathbf{B}$.  

**Write out the expressions for the linear predictor matrix $\mathbf{A}$ and the non-linear hidden unit values $\mathbf{H}$ assuming a logistic (inverse logit) function is used.**

#### SOLUTION

Add as many equation blocks as you feel are necessary.  

$$
\mathbf{A} = \mathbf{X}\mathbf{B}
$$
$$
\mathbf{H} = \mathbf{logit}^{-1}(\mathbf{X}\mathbf{B})
$$

### 1d)

You will now define a function which calculates the hidden unit "linear predictor" and non-linear values. The function is named `calc_hidden_units()` and it has three input arguments. The first, `X`, is the input design matrix, the second, `B`, is the hidden unit parameter matrix, and the third is the non-linear transformation function. The transformation function argument is named `g` to be consistent with the lecture notation of $g\left( \cdot \right)$.  

This function is general and therefore allows passing in an arbitrary non-linear transformation function.  

**Complete the code chunk below. Calculate the linear predictor matrix `A` and the non-linear transformed hidden unit matrix `H`. The results are returned in a list for you.**  

#### SOLUTION

```{r, solution_01d, eval=TRUE}
calc_hidden_units <- function(X, B, g)
{
  ### your code
  A <- X %*% B
  H <- g(A)
  
  ### book keeping
  return(list(A = A, H = H))
}
```

### 1e)

You will calculate the hidden units for the `prob_01_df` dataset and the `params_01_2a` parameters. The code chunk below provides a function which visualizes the hidden units with respect to a single input, `x`. You will use this function to interpret the behavior of the hidden units. `viz_hidden_trend_wrt_x()` accepts three input arguments. The first, `v_mat`, is a matrix of hidden unit values. The second, `x_df`, is a `data.frame` which must contain a column named `x`. The third, `trend_type`, is a character string containing the "type" of hidden unit values contained in the `v_mat` matrix. The `trend_type` variable is assigned to the legend title. The `trend_type` argument is used to state whether the resulting figure is plotting the hidden unit "linear predictors" or the non-linear hidden unit values.  


```{r, define_hidden_viz_func}
viz_hidden_trend_wrt_x <- function(v_mat, x_df, trend_type)
{
  x_df %>% 
    select(all_of(c("x"))) %>% 
    tibble::rowid_to_column("obs_id") %>% 
    left_join(v_mat %>% as.data.frame() %>% 
                tibble::as_tibble() %>% 
                purrr::set_names(sprintf("h_%02d", 1:ncol(v_mat))) %>% 
                tibble::rowid_to_column("obs_id"),
              by = "obs_id") %>% 
    pivot_longer(!c("obs_id", "x")) %>% 
    ggplot(mapping = aes(x = x, y = value)) +
    geom_vline(xintercept = 0, color = 'grey50') +
    geom_line(mapping = aes(color = name, group = name),
              size = 1.15) +
    ggthemes::scale_color_colorblind(trend_type) +
    theme_bw() +
    theme(legend.position = "top")
}
```


**IMPORTANT**: In this problem you will use the **logistic** function as the non-linear (activation) function.  

**Complete the two code chunks below. You must create the design matrix `X01`. You must call the `calc_hidden_units()` function by passing in the appropriate arguments and storing the result to `nnet_hidden_2a`. Two calls to `viz_hidden_trend()` are started for you. Complete the calls by assigning the correct element from `nnet_hidden_2a`, `A` or `H`, as the first argument to the `viz_hidden_trend()` call.**  

*HINT*: The third argument to `viz_hidden_trend_wrt_x()` tells you to whether to assign the "linear predictors" or the non-linear hidden unit values.  

*HINT*: Use the `$` operator to access the elements from the list `nnet_hidden_2a`.  

*HINT*: What function can we use for the logistic function?  

#### SOLUTION

Logit transformation

```{r, solution_01e_a, eval=TRUE}
X01 <- model.matrix( ~ x, prob_01_df)

nnet_hidden_2a <- calc_hidden_units(X01, params_01_2a$beta_matrix, boot::inv.logit)

nnet_hidden_2a
```

Visualize the hidden unit trends below.  

```{r, solution_01e_b, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_2a$A,
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_2a$H,
                       prob_01_df,
                       "hidden non-linear units")
```

### 1f)

**Describe the trends of the hidden linear predictors relative to the $\beta$ parameter values displayed in Problem 1b).**  

#### SOLUTION

What do you think?  

Through the trend of hidden unit, the slope of unit 1 is positive and the slope of unit 2 is negative. When x = 0, the value of output represent the value of intercept of two units, which are pretty close to 0.

### 1g)

The neural network response, $f$, is calculated as a linear combination of the non-linear hidden unit values in the output layer. Assume the output layer consists of an intercept (bias) $\alpha_{0}$ and a column vector of slopes (weights) $\boldsymbol{\alpha}$.  

**Write out the expression for the response vector $\mathbf{f}$ given the output layer parameters, $\alpha_{0}$, $\boldsymbol{\alpha}$, and the matrix of non-linear hidden unit values $\mathbf{H}$.**  

#### SOLUTION

Add as many equation blocks as you feel are necessary.  

$$
\mathbf{f} = \alpha_{0} + \boldsymbol{\alpha} \mathbf{H}
$$

### 1h)

As with the hidden units, you will use a function to calculate the neural network response. The code chunk below defines the `calc_nnet_response()` function. It accepts two arguments. The first, `H`, is the matrix of non-linear hidden unit values and the second, `a`, is the "regular vector" of output layer parameters. Note that `a` contains the intercept (the bias) and the slopes (the weights).  

**Complete the code chunk below. You must separate the `a` vector into the intercept (bias) and the slopes (weights). Store the intercept as the `a_0` variable and the slopes as the `a_w` regular vector. You must then convert the `a_w` regular vector into a column vector `a_col`. Finally, you must calculate the response, `f`. The response is returned as a vector for you.**  

#### SOLUTION

```{r, solution_01h, eval=TRUE}
calc_nnet_response <- function(H, a)
{
  ### separate the vector into bias and weights
  a0 <- a[1]
  a_w <- a[2:length(a)]
  
  # convert the weights to a column vector
  a_col <- as.matrix(a_w)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% a_col
  
  as.vector(f)
}
```

### 1i)

With all the calculations completed, let's now compare the neural network response to the true sine wave.  

**Complete the code chunk below by assigning the correct the arguments to the `calc_nnet_response()` function. The rest of the code, which generates the figure, is completed for you.**  

**How would you describe the neural network's fit? Which portions are approximated well?**  

#### SOLUTION

```{r, solution_01i, eval=TRUE}
prob_01_df %>% 
  mutate(nnet_f = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector)) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = nnet_f),
            color = "black", size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", shape = 1, size = 3.5) +
  labs(y = "f") +
  theme_bw()
```

What do you think: How would you describe the neural network's fit? Which portions are approximated well?

Generally speaking, most of the trend were captured by our prediction. The interval between -1 and 1 is the best. 

## Problem 02

You will now repeat your calculations from Problem 1. You will first try out a different set of parameters for a two hidden unit neural network. Then you consider a neural network with 5 hidden units.  

### 2a)

The code chunk below reads in a new set of neural network parameters. The format is consistent with that from Problem 1, in that the hidden unit parameters are contained within the element `$beta_matrix` and the output layer parameters are stored in `$alpha_vector`. The parameters are printed to the screen below.  

```{r, read_in_prob_02a_params_b}
url_load_01_2b <- paste(paste(url_load_dir, "hw_10_prob_01_params_2b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2b <- readr::read_rds( url_load_01_2b )

params_01_2b
```

You will use these new parameters to calculate the hidden unit "linear predictors" and the non-linear hidden unit values for the two hidden unit neural network.  

**Complete the code chunk below. Call the `calc_hidden_units()` function with the appropriate arguments and assign the result to the `nnet_hidden_2b` object. Then assign the correct arguments to the `viz_hidden_trend()` functions calls. Are the trends of the hidden unit values (linear and non-linear) consistent with the trends from Problem 1? If not, what would be causing the change? Based on the figures generated in the code chunk below, do you think the resulting neural network will be different from that in Problem 1?**  

#### SOLUTION

```{r, solution_02, eval=TRUE}
X02 <- model.matrix( ~ x, prob_01_df)

nnet_hidden_2b <- calc_hidden_units(X02, params_01_2b$beta_matrix, boot::inv.logit)

viz_hidden_trend_wrt_x(nnet_hidden_2b$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_2b$H,
                       prob_01_df,
                       "hidden non-linear units")
```

What do you think?

They are different. As we can see through figures, hidden unit 1 is increasing in first model while now its decreasing. The change of beta parameter is the reason of causing this kind of change. Despite this change, we can't say the final result would be different or same because we have weights(coefficients) that are tuning the weights of each input over the final result.

### 2b)

Let's now compare the responses associated with the two sets of parameters.  

**Complete the first code chunk below by assigning the correct arguments to the two `calc_nnet_response()` calls. The first call is associated with the parameters from Problem 1, with the result stored to the `nnet_fa` variable. The second call is associated with the new set of parameters, and the result is stored to the `nnet_fb` variable.**  

**The second code chunk is completed for you. It plots the two responses together with the true sine wave output. Based on the figure below, how do the two different neural network models compare?**  

#### SOLUTION

```{r, solution_02b_a, eval=TRUE}
results_2hidden <- prob_01_df %>% 
  mutate(nnet_fa = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector),
         nnet_fb = calc_nnet_response(nnet_hidden_2b$H, params_01_2b$alpha_vector))
```

Now visualize the neural network predictions and compare to the true sine wave.  

```{r, solution_02b_b, eval=TRUE}
results_2hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

What do you think?  

Even though the first hidden unit appears differently in two models, but the final result are almost the same!

### 2c)

**Based on your results, can you "explain" or interpret the neural network behavior by ONLY examining the slopes (weights) acting on the inputs?**  

#### SOLUTION

What do you think?  

No, the weights of inputs is only a part of affection. Final result is still tuned by non-linear predictor weights.

### 2d)

Let's now perform the same type of calculations, but on a neural network with 5 hidden units instead of 2. As before, you will compare two sets of parameters for the 5 hidden unit model. The code chunk below reads in the two different sets of neural network parameters. The format is consistent with that from Problem 1, except now there are more parameters since there are more hidden units.  


```{r, read_in_prob_2_5_hidden_params}
url_load_01_5a <- paste(paste(url_load_dir, "hw_10_prob_01_params_5a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5a <- readr::read_rds( url_load_01_5a )

url_load_01_5b <- paste(paste(url_load_dir, "hw_10_prob_01_params_5b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5b <- readr::read_rds( url_load_01_5b )
```

The first set neural network parameters, `params_02_5a`, are displayed for you below to show the difference in structure with the 2 hidden unit neural networks you worked with previously.  

```{r, show_prob_2_5_hidden_params_a}
params_02_5a
```


**Calculate the hidden unit "linear predictors" and non-linear values for the two different 5 hidden unit models. Then complete the calls to the `viz_hidden_trend_wrt_x()` function to plot the hidden unit trends with respect to the input.**  

**Describe the trends of the non-linear values for the fifth hidden unit, `h_05`, relative to its "linear predictor" values. Discuss the differences between the two models (sets of parameters).**  

#### SOLUTION

Calculate the hidden unit "linear predictors" and non-linear hidden unit values.  

```{r, solution_02d_a, eval=TRUE}
X05 <- model.matrix( ~ x, data = prob_01_df)
nnet_hidden_5a <- calc_hidden_units(X05, params_02_5a$beta_matrix, boot::inv.logit)

nnet_hidden_5b <- calc_hidden_units(X05, params_02_5b$beta_matrix, boot::inv.logit)
```

Visualize the behavior of the hidden units associated with the `params_02_5a` parameters.  

```{r, solution_02d_b, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_5a$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_5a$H, 
                       prob_01_df,
                       "hidden non-linear units")
```

Visualize the behavior of the hidden units associated with the `params_02_5b` parameters.  

```{r, solution_02d_c, eval=TRUE}
viz_hidden_trend_wrt_x(nnet_hidden_5b$A, 
                       prob_01_df,
                       "hidden linear predictors")

viz_hidden_trend_wrt_x(nnet_hidden_5b$H, 
                       prob_01_df,
                       "hidden non-linear units")
```

What do you think?  

As we can see through figures above, for the 1st set of parameters, in linear relationship, output increase all the way bigger than 20 as the input increase. But non-linear output is still within 1 to 0 interval. That is because of logistic transformation. As to the 2nd set of parameters, in linear relationship, the output decrease as the input increase. Non-linear output is still bounded within 0 to 1 because of logistic transformation.

### 2e)

Let's now calculate the neural network response for both with 5 hidden units.  

**Complete the first code chunk below by assigning the arguments correctly to the two `calc_nnet_response()` function calls. The first call is intended for the model associated with `params_02_5a` while the second call is intended for the model associated with `params_02_5b`.**  

**The second code chunk is completed for you. How do the two models compare to each other and to the true sine wave?**  

#### SOLUTION

```{r, solution_02e_a, eval=TRUE}
results_5hidden <- prob_01_df %>% 
  mutate(nnet_fa = calc_nnet_response(nnet_hidden_5a$H, params_02_5a$alpha_vector),
         nnet_fb = calc_nnet_response(nnet_hidden_5b$H, params_02_5b$alpha_vector))
```

```{r, solution_02e_b, eval=TRUE}
results_5hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

What do you think: How do the two models compare to each other and to the true sine wave?

Generally speaking, both models fit predict the true sine wave very well. There difference are minor. If there has to be a better one, the black curve might be a little bit better.

### 2f)

The code chunk below is completed for you. The 2 hidden unit and 5 hidden unit model predictions are compared side by side.  

```{r, solution_02f_a, eval=TRUE}
results_2hidden %>% 
  mutate(num_hidden = 2) %>% 
  bind_rows(results_5hidden %>% 
              mutate(num_hidden = 5)) %>% 
  pivot_longer(!c("x", "f", "num_hidden")) %>% 
  tidyr::separate(name,
                  c("nnet_word", "fparams"),
                  sep = "_") %>% 
  tidyr::separate(fparams,
                  c("fltr", "paramset"),
                  sep = 1) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = interaction(paramset,
                                              num_hidden),
                          linetype = paramset,
                          color = paramset),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  facet_grid( ~ num_hidden, labeller = "label_both") +
  ggthemes::scale_color_colorblind("Params") +
  scale_linetype_discrete("Params") +
  labs(y = "f") +
  theme_bw()
```

**Based on the figure above, which model, the 2 hidden units or 5 hidden units, performs better? What controls complexity within a neural network model and how could you go about "tuning" that complexity?**  

#### SOLUTION

What do you think?

As we can see through figures, 5 hidden units models perform better than 2 hidden units model. Because more neurons means more flexibility, which is also the key factor of controlling the complexity of the model. To tune it, we can use cross-validation.

### 2g)

The toy data within Problem 1 and 2 comes from a simple sine wave:  

$$ 
f\left(x\right) = \mathrm{sin}\left(x\right)
$$

Let's see if a linear model, using the correct `sin()` basis can correctly identify that the "slope" acting on `sin(x)` is 1.  

**Use the `lm()` function to a fit a linear model for the response `f` and the sine of the input, `sin(x)`. Use the `prob_01_df` data set as the `data` argument to the `lm()` call. Assign the result to txhe `lm_sine_mod` object.**  

**Print the `summary()` of the `lm()` call to the screen. What is the estimate for the slope associated with the `sin(x)`?**  

#### SOLUTION

```{r, solution_02g, eval=TRUE}
lm_sine_mod <- lm(f ~ sin(x), prob_01_df)

### print the summary to the screen
lm_sine_mod %>% summary()
```

What is the estimate to the slope acting on `sin(x)`?  

Intercept = 0 and slope = 1

### 2h)

**How many unknown parameters were there in the linear model with the sine wave basis function? How many unknown parameters existed in the neural network with 5 hidden units?**  

#### SOLUTION

What do you think?

2 unknown parameter in linear model: intercept and coefficient. To a neural network with 5 hidden units and 1 input, we have (5 * 2 + 5 + 1 = 16) 16 unknown parameters.

### 2i)

**It was really simple to fit the linear model. Why would we want to use a neural network when we can build the exact model in this application using `lm()`?**  

#### SOLUTION

What do you think?

Because neural network can help "learning" the trend since in a real problem, there's no way we can know the real formula in advance. In neural network, we can apply linear function to produce neurons/hidden units and apply non-linear function to produce final result, which provide more flexibility to learn and predict the real trend. 

## Problem 03

In the previous problems, you focused on the predictions of a neural network. You will now work through fitting neural networks, on a slightly more realistic example. The code chunk below reads a data set consisting of 3 continuous inputs, `x1`, `x2`, and `x3`, and a continuous response, `y`. A glimpse of the data set is displayed to the screen for you.  

```{r, read_prob_03_data}
url_03_train <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_03_train.csv'

prob_03_df <- readr::read_csv( url_03_train, col_names = TRUE)

prob_03_df %>% glimpse()
```

### 3a)

The wide-format data set is converted into a long-format data set for you in the code chunk below. The glimpse displayed to the screen shows that the inputs have been "stacked" or "gathered" together into a column `name` with their values given in the `value` column.  

```{r, make_03_longformat}
prob_03_lf <- prob_03_df %>% 
  tibble::rowid_to_column("obs_id") %>% 
  pivot_longer(!c("obs_id", "y"))

prob_03_lf %>% glimpse()
```

**Plot the noisy response, `y`, with respect to each input using the long-format data set. Using the `geom_point()` geom and create separate facets (subplots) for each input using `facet_wrap()`.**  

**What trends do you see in the scatter plots?**  

#### SOLUTION

To be honest, I can't see any trends. The output might be increasing as the input increases in x1 and x2 while the output decrease as the input increases in x3

```{r, solution_03a}
###
prob_03_lf %>%
  ggplot(mapping = aes(x = value, y = y))+
  geom_point()+
  facet_wrap(~name)
```

### 3b)

In the previous problems you used a logistic function as the non-linear function associated with each hidden unit. However, there are many different functions that could be used. To get exposure working with a different "activation" function, you will use the hyperbolic tangent function for this problem. Let's first get an idea about how the hyperbolic tangent compares with the logistic function.  

**Complete the code chunk below by setting `x` to be 101 equally spaced points between -5.5 and 5.5. Calculate the the logistic function of `x` and assign the result to `logistic_result`. Calculate the hyperbolic tangent of `x` and assign the result to `tanh_result`. The result of the code chunk is completed for you. It visualizes the non-linear transformations with respect to the `x` variable.**  

**Is the hyperbolic tangent function similiar to the logistic function? In what ways are the two different?**  

*HINT*: The hyperbolic tangent function in `R` is `tanh()`.  

```{r, solution_03b, eval=TRUE}
tibble::tibble(
  x = seq(-5.5, 5.5, length.out = 101)
) %>% 
  mutate(logistic_result = boot::inv.logit(x),
         tanh_result = tanh(x)) %>% 
  # rest of the code here is completed for you
  pivot_longer(!c("x")) %>% 
  ggplot(mapping = aes(x = x, y = value)) + 
  geom_hline(yintercept = c(-1, 0, 1),
             color = 'grey50', linetype = 'dashed') +
  geom_line(mapping = aes(y = value,
                          color = name,
                          linetype = name),
            size = 1.15) +
  ggthemes::scale_color_calc("") +
  scale_linetype_discrete("") +
  theme_bw() +
  theme(legend.position = "top") 
```
Is the hyperbolic tangent function similiar to the logistic function? In what ways are the two different?

They are not identical same. Tangent curve includes negative value while logistic is from 0 to 1. But both of them are s-curve.

### 3c)

You will fit the neural network by minimizing the sum of squared errors (SSE). Thus, we will work with a non-probabilistic setting, even though we derived the linear and generalized linear model fitting with likelihoods and priors. Remember that minimizing the SSE is analogous to maximizing a Gaussian log-likelihood!  

**Write the expression for the SSE using the observed response $y$ and the neural network response $f$. You may write the SSE in either the summation or matrix/vector notation. If you use a summation notation use the subscript $n$ to denote a single observation. If you use matrix/vector notation denote the response vector as $\mathbf{y}$.**  

#### SOLUTION

Add as many equation blocks as you feel are necessary.  

$$
SSE = \sum_{n=1}^{N}((\ y_{n} - \ f_{n})^2)
$$

### 3d)

You will now program the error function we wish to minimize in the style of the log-posterior functions from earlier homework assignments. You will name your function `my_neuralnet_sse()`. It will consist of two input arguments, a vector of parameters to learn and a list of required information. Before defining the function, you will create the list of required information, which is started for you in the code chunk below. Notice that the structure is similar to the lists of information created for the generalized linear models. However, two pieces of information not associated with GLMs are required for the neural network. The variable `$num_hidden` specifies the number of hidden units and the variable `$transform_hidden` stores the non-linear transformation function to apply to each hidden unit.  

You will start out with a small neural network consisting of 3 hidden units. You will use the hyperbolic tangent as the non-linear transformation function, instead of the logistic function that you worked with in the previous problems. You will therefore need to assign the `tanh()` function correctly to the `$transform_hidden` field in the list. Be careful about the `()` when assigning the function *object*!  

You will need to specify the design matrix for your neural network based on the 3 inputs in the `prob_03_df` data set. Think carefully how the design matrix is structured in a neural network.  

**Complete the list of required information by completing the code chunk below. You must create the design matrix based on the three inputs. Assign the design matrix to the `$design_matrix` variable in the list. Assign the observed responses to the `$yobs` variable in the list. Set the number of hidden units to be 3.**  

**After specifying the `info_three_units` list, calculate the total number of parameters in the single hidden layer neural network with 3 hidden units and assign the result to the `info_three_units$num_params` variable.**  

#### SOLUTION

The code chunk is started for you below.  

```{r, solution_03d, eval=TRUE}
### design matrix
Xmat_03 <- model.matrix(y ~ x1 + x2 + x3, prob_03_df)

info_three_units <- list(
  yobs = prob_03_df$y,
  design_matrix = Xmat_03,
  num_hidden = 3,
  transform_hidden = tanh
)

info_three_units$num_params <- info_three_units$num_hidden * ncol(Xmat_03) + info_three_units$num_hidden + 1
```

The total number of hidden units you calculated in the above code chunk are printed to the screen below.  

```{r, solution_03d_b, eval=TRUE}
info_three_units$num_params
```


### 3e)

You will now define the $SSE$ objective in the `my_neuralnet_sse()` function below. As described previously, the function consists of two input arguments. The first argument, `theta`, contains all of the unknown parameters to learn. The vector is organized with all hidden unit parameters listed before the output layer parameters. The first part of the `my_neuralnet_sse()` function has several portions completed for you. **You are responsible for determining the number of hidden unit parameters (the betas) for each hidden unit.** You should **not** hard code `length_beta_per_unit`. You must then calculate the total number of hidden unit parameters and assign the result to `total_num_betas`. Again you should **not** hard code this number because later on you will try out more hidden units.  

The hidden unit parameters are extracted from the `theta` vector and organized into the `Bmat` matrix with dimensions consistent with the $\mathbf{B}$ described in Problem 01 and 02.  

The output layer parameters are extracted for you and assigned to the `a_all` vector. You must reorganize the output layer parameters by separating the bias, `a0`, and output layer weights, `aw`. The bias should be a scalar quantity and the output layer weights should be a "regular vector".  

You must complete the function by performing the necessary matrix math calculations, transformations, and calculation of the $SSE$. The comments in the function describe what you must complete in each line.  

After completing the function, test that it works using two separate guesses for the unknown parameters. First set all parameters to a value of 0, then set all parameters to a value of -1.25. If your function is specified correctly the $SSE$ should be `683.113` for the guess of all 0's and it should be `2238.39` for the guess -1.25 for all parameters.  

**Complete the `my_neuralnet_sse()` function below and test it's operation with the two guesses specified in the problem statement.**  

#### SOLUTION

The `my_neuralnet_sse()` function is started for you in the code chunk below.  

```{r, solution_03e, eval=TRUE}
my_neuralnet_sse <- function(theta, my_info)
{
  # extract the hidden unit parameters
  X <- my_info$design_matrix
  length_beta_per_unit <- ncol(X)
  total_num_betas <- my_info$num_hidden * length_beta_per_unit
    
  beta_vec <- theta[1:total_num_betas]
  
  # reorganize the beta parameters into a matrix
  Bmat <- matrix(beta_vec, nrow = length_beta_per_unit, byrow = FALSE)
  
  # extract the output layer parameters
  a_all <- theta[(total_num_betas + 1):length(theta)]
  
  # reorganize the output layer parameters by extracting
  # the output layer intercept (the bias)
  a0 <- a_all[1]
  aw <- a_all[2:length(a_all)]
  
  # calculate the linear predictors associated with
  # each hidden unit
  A <- X %*% Bmat
  
  # pass through the non-linear transformation function
  H <- my_info$transform_hidden( A )
  
  # calculate the response (the output layer)
  f <- as.vector(a0 + H %*% as.matrix(aw))
  
  # calculate the SSE
  return(sum((my_info$yobs - f)^2))
}
```

Test out your `my_neuralnet_sse()` function with values of 0 for all parameters.  

```{r, solution_03e_b}
my_neuralnet_sse(rep(0, info_three_units$num_params), info_three_units)
```

Test out your `my_neuralnet_sse()` function with values of -1.25 for all parameters.  

```{r, solution_03e_c}
my_neuralnet_sse(rep(-1.25, info_three_units$num_params), info_three_units)
```

### 3f)

With the objective function completed, it's now time to fit the simple neural network with 3 hidden units. You will use the `optim()` function to perform the optimization, just as in the previous assignments. Since we are focused on finding the estimates at the moment, you will work with `optim()` itself, rather than within the `my_laplace()` wrapper as in previous assignments.  

You will fit two neural networks from two different starting guess values. The first starting guess will be a vector of 0's, and the second guess will be -1.25 for all parameters. Complete the two code chunks below by specifying the initial guesses correctly and completing the remaining input arguments to the `optim()` call. You must set the `gr` argument to `r NULL` so that `optim()` uses finite differences to estimate the gradient vector. Pass in the `info_three_units` list of required information to both `optim()` calls. Specify the `method` argument to be `"BFGS"` to use the quasi-Newton BFGS algorithm. Set the `hessian` argument to be `r FALSE` which forces the Hessian matrix to **NOT** be estimated at the end. We are simply interested in the point estimates at the moment and so we will not be concerned with the curvature of the error surface. The maximum number of iterations is set for you in both `optim()` calls already.  

**Complete both code chunks below in order to fit the three hidden unit neural network with two different starting guesses. Follow the instructions in the problem statement to specify all the arguments to the `optim()` calls.**  

**After fitting, print out the identified optimal parameters contained in the `$par` field of the `optim()` results for both cases. Are the identified optimal parameter values the same between the two starting guesses? Why would the results not be the same?**  

#### SOLUTION

Fit the neural network with the initial guess of 0's for all parameters.  

```{r, solution_03f_a, eval=TRUE}
optim_fit_3_a <- optim(rep(0, info_three_units$num_params),
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method =  "BFGS",
                       hessian =  FALSE,
                       control = list(maxit = 5001))
```

Fit the neural network with the initial guess of -1.25 for all parameters.  

```{r, solution_03f_b, eval=TRUE}
optim_fit_3_b <- optim(rep(-1.25, info_three_units$num_params),
                       my_neuralnet_sse,
                       gr =  NULL,
                       info_three_units,
                       method =  "BFGS",
                       hessian =  FALSE,
                       control = list(maxit = 5001))
```

Compare the optimized parameter estimates.  

```{r, solution_03f_c}
###optim_fit_3_a
optim_fit_3_a$par
optim_fit_3_b$par
```

Are the identified optimal parameter values the same between the two starting guesses? Why would the results not be the same? What do you think?  

No, they are not the same. Because the the distribution of our parameters is NOT unimodal, which means we have several peak. Thus, different initial guess will affect the optimization result. With some guesses, result will be stuck at local peak, which misses global peak. Others may find the global peak. 

### 3g)

Fit the neural network with 3 hidden units again, but this time use 2 randomly generated initial guess values. Use standard normals (mean 0 and standard deviation 1) to generate the initial guesses.  

**Complete the two code chunks below by generating two random initial guess values. Assign the first random initial guess to `init_guess_03_c` and the second random initial guess to `init_guess_03_d`.**  
**Complete the `optim()` calls following the same instructions as the previous question.**  

**Check if the optimized parameter estimates are the same or not.**  

#### SOLUTION

Set the random initial guess values.  

```{r, solution_03g_a, eval=TRUE}
set.seed(412412)
init_guess_03_c <- rnorm(info_three_units$num_params, mean = 0, sd = 1)
  
set.seed(214214)
init_guess_03_d <- rnorm(info_three_units$num_params, mean = 0, sd = 1)
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_b, eval=TRUE}
optim_fit_3_c <- optim(init_guess_03_c,
                       my_neuralnet_sse,
                       gr = NULL,
                       info_three_units,
                       method =  "BFGS",
                       hessian =  FALSE,
                       control = list(maxit = 5001))
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_c, eval=TRUE}
optim_fit_3_d <- optim( init_guess_03_d,
                        my_neuralnet_sse,
                       gr =  NULL,
                        info_three_units,
                       method =  "BFGS",
                       hessian =  FALSE,
                       control = list(maxit = 5001))
```

Compare the parameter estimates.  

```{r, solution_03g_d}
###
optim_fit_3_c$par
optim_fit_3_d$par
```

Check if the optimized parameter estimates are the same or not.

No, they are not the same

### 3h)

The `optim()` results store the objective function value as the `$value` field in the returned list object.  

**Compare the SSE for the 4 different starting guesses. Which model is better, as viewed by the training set?**  

#### SOLUTION

```{r, solution_03h, eval=TRUE}
sseData = c(optim_fit_3_a$value, optim_fit_3_b$value, optim_fit_3_c$value, optim_fit_3_d$value)
sseData
```

Compare the SSE for the 4 different starting guesses. Which model is better, as viewed by the training set?

Model b,c and d share almost the same SSE, they are basically as good as each other and better than Model a.

## Problem 04

You now have the major pieces in place for fitting neural networks! In this problem, we will fit additional neural networks with more hidden units!  

### 4a)

Let's define a function which will generate a random initial guess for the appropriate number of unknown parameters and then execute the `optim()` call. The `train_1layer_nnet_sse()` function has 4 input arguments. The first argument, `num_hidden`, is the number of hidden units in the hidden layer, the second, `transform_func`, is the non-linear transformation (activation) function, the third `X`, is the design matrix, and the fourth, `y`, the response vector.  

**Complete the code chunk below which assembles the list of required information and generates the random initial guess, for an arbitrary number of hidden units in the first hidden layer. Do not set the random seed inside the `train_1layer_nnet_sse()` function. We will set the seed before we fit the models.**  

*HINT*: If your function below is setup correctly you should be able to replicate the previous results if the **SAME** random seed is used. The second code chunk below resets the random seed for you as a confirmation test.  

#### SOLUTION

```{r, solution_04a, eval=TRUE}
train_1layer_nnet_sse <- function(num_hidden, transform_func, X, y)
{
  my_info_list <- list(
    yobs = y,
    design_matrix = X,
    num_hidden = num_hidden,
    transform_hidden = transform_func
  )
  
  my_info_list$num_params <- num_hidden * ncol(X) + num_hidden + 1
  
  # generate random initial guess
  init_guess <- rnorm(n = my_info_list$num_params, mean = 0, sd = 1)
  
  # call optim to fit the neural network
  optim( init_guess,
         my_neuralnet_sse,
         gr =  NULL,
         my_info_list,
         method =  "BFGS",
         hessian =  FALSE,
         control = list(maxit = 10001))
}
```

As a check fit the 3 hidden unit neural network again with the same random seed as used with `init_guess_03_c`. You should get the same parameters as `optim_fit_3_c`.  

```{r, solution_04a_b, eval=TRUE}
set.seed(412412)
check_optim_fit_3_c <- train_1layer_nnet_sse(3, tanh, Xmat_03, prob_03_df$y)
```

Compare to the previous `optim_fit_3_c` results.  

```{r, solution_04a_c}
### 
all.equal(check_optim_fit_3_c$par, optim_fit_3_c$par)
```


### 4b)

Let's now fit neural networks with 6, 12, and 24 hidden units instead of 3 hidden units. You will use two different initial guesses for each hidden layer size. The random seeds are set for you to make sure the results are reproducible.  

**Complete the three code chunks below by setting the input arguments to fit 2 pairs of 6, 12, and 24 hidden unit neural networks.**  

#### SOLUTION

```{r, solution_04b, eval=TRUE}
set.seed(412412)
optim_fit_6_a = train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_6_b = train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)
```

```{r, solution_04b_b, eval=TRUE}
set.seed(412412)
optim_fit_12_a = train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_12_b = train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)
```

Please note that fitting the two 24 hidden unit neural networks may take a few minutes.  

```{r, solution_04b_c, eval=TRUE}
set.seed(412412)
optim_fit_24_a = train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_24_b = train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)

```


### 4c)

Compare the training set SSE across all of the models you trained with random initial guesses (including the 3 hidden unit models).  

**Which model was considered the best according to the training set?**  

#### SOLUTION

Add as many code chunks as you feel are necessary to answer this question.  

```{r, solution_04c}
optim_fit_3_c$value
optim_fit_3_d$value
optim_fit_6_a$value
optim_fit_6_b$value
optim_fit_12_a$value
optim_fit_12_b$value
optim_fit_24_a$value
optim_fit_24_b$value
```

Which model was considered the best according to the training set?

According to the result, the model with 24 hidden units has the overall best performance.

## Problem 05

We know that we should **not** compare models strictly based on the training set (unless we were using an information criterion metric). The code chunk below reads in a hold-out test set for you. This hold out test set will be used to compare the performance across the hidden layer sizes that you have fit so far.  

```{r, read_holdout_set}
url_03_test <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_03_test.csv'

prob_03_test_df <- readr::read_csv( url_03_test, col_names = TRUE)

prob_03_test_df %>% glimpse()
```

### 5a)

You will define a function which makes predictions for you and calculates the Mean Squared Error (MSE) on the test set. The function, `assess_nnet_mse()`, is started for you in the code chunk below. The first argument, `theta`, is a vector of all unknown parameters, the second argument, `num_hidden`, is the number of hidden unit parameters, the third argument, `transform_func`, is the non-linear transformation function, the fourth argument, `X`, is a design matrix, and the fifth argument, `y`, is the response vector.  

**You have worked with the necessary pieces to complete this function several different ways in this assignment. You are free to decide how best to calculate the MSE for a given set of parameters. The only requirement is that `assess_nnet_mse()` should return a scalar number.**  

#### SOLUTION

```{r, solution_05a, eval=TRUE}
assess_nnet_mse <- function(theta, num_hidden, transform_func, X, y)
{
  my_info_list <- list(
    yobs = y,
    design_matrix = X,
    num_hidden = num_hidden,
    transform_hidden = transform_func
  )
  
  my_info_list$num_params <- length(theta)
  
  return(my_neuralnet_sse(theta, my_info_list) / length(y))
}
```

### 5b)

Before you can calculate the MSE on the hold-out test set, you must create the test design matrix.  

**Create the appropriate test design matrix associated with all 3 inputs, using the `prob_03_test_df` data set, and assign the result to `Xtest_03`.**  

#### SOLUTION

```{r, solution_05b, eval=TRUE}
Xtest_03 = model.matrix(y ~ x1+x2+x3, data = prob_03_test_df)
```

### 5c)

**Calculate the MSE for each of the models trained with random initial guess values. You are free to decide how to execute this task.**  

#### SOLUTION

Add as many code chunks as you feel are necessary to complete the problem.  

```{r, soluion_05c}

mse_model3a <- assess_nnet_mse(optim_fit_3_c$par, 3, tanh, Xtest_03, prob_03_test_df$y)
mse_model3b <- assess_nnet_mse(optim_fit_3_d$par, 3, tanh, Xtest_03, prob_03_test_df$y)
mse_model6a <- assess_nnet_mse(optim_fit_6_a$par, 6, tanh, Xtest_03, prob_03_test_df$y)
mse_model6b <- assess_nnet_mse(optim_fit_6_b$par, 6, tanh, Xtest_03, prob_03_test_df$y)
mse_model12a <- assess_nnet_mse(optim_fit_12_a$par, 12, tanh, Xtest_03, prob_03_test_df$y)
mse_model12b <- assess_nnet_mse(optim_fit_12_b$par, 12, tanh, Xtest_03, prob_03_test_df$y)
mse_model24a <- assess_nnet_mse(optim_fit_24_a$par, 24, tanh, Xtest_03, prob_03_test_df$y)
mse_model24b <- assess_nnet_mse(optim_fit_24_b$par, 24, tanh, Xtest_03, prob_03_test_df$y)

```

### 5d)

**Which model is the best as viewed by the hold-out test set performance?**  

#### SOLUTION

```{r, solution_05d}
mse_model3a
mse_model3b
mse_model6a
mse_model6b
mse_model12a
mse_model12b
mse_model24a
mse_model24b
```

What do you think?  

According to the data, models with 6 hidden units have the lowest MAE value, which means perform the best and follows by model with 12 and 3 hidden units. Besides, 24 hidden units model is the worst. 

## Problem 06

You not only fit neural networks from scratch, but you used a hold-out test set to **tune** the number of hidden units! Doing so required you to directly work with the assumptions of the neural network, learning how to make predictions with the matrix operations, calculate the performance metric, and ultimately assess the potential for overfitting as the complexity increases. Understanding the assumptions and concepts are critical when assessing the behavior and performance of a neural network in a practical application when we use existing functions and packages to fit the neural network for us. In this last problem you will practice using `caret` to manage the training, evaluation, and tuning of a neural network. You will use the `nnet` package to fit the neural network. Please download and install `nnet` if you do not have it already. If you do not install it, `caret` will prompt you to install it and so please check the R console if nothing seems to happen when you use the `caret::train()` function.  

The code chunk below loads the `caret` package for you. You do not need to load `nnet`, the `caret` package will manage that for you.  

```{r, load_caret_package}
library(caret)
```

### 6a)

You must specify the resampling scheme that `caret` will use to train, assess, and tune the model.  

**Specify the resampling scheme to be 5 fold with 3 repeats. Assign the result of the `trainControl()` function to the `my_ctrl` object. Specify the primary performance metric to be `'RMSE'` and assign that to the `my_metric` object.**  

#### SOLUTION

```{r, solution_06a, eval=TRUE}
### your code here
my_ctrl <- trainControl(method = 'repeatedcv', number = 5, repeats = 3, savePredictions = TRUE)
my_metric <- 'RMSE'
```

### 6b)

In a realistic application, it is always best to first fit linear models before we fit neural networks. The linear models (especially regularized models which include interaction features) serve as interpretable baseline models. However, since this assignment is focused on neural networks we will just fit the neural network. You must train, assess, and tune a neural network using the **default** `caret` tuning grid. In the `caret::train()` function you must use the formula interface to specify the inputs are `x1`, `x2`, and `x3`, while the response is `y`. Assign the `method` argument to `'nnet'` and set the `metric` argument to `my_metric`. You must also instruct `caret` to standardize the features by setting the `preProcess` argument equal to `c('center', 'scale')`. Assign the `trControl` argument to the `my_ctrl` object.  

**Train, assess, and tune the `nnet` neural network with the defined resampling scheme. Assign the result to the `nnet_default` object and print the result to the screen. Which tuning parameter combinations are considered to be the best?**  

**IMPORTANT**: include the argument `trace = FALSE` in the `caret::train()` function call. This will make sure the `nnet` package does NOT print the optimization iteration results to the screen.  

#### SOLUTION

```{r, solution_06b, eval=TRUE}
set.seed(412412)
nnet_default <- train(y ~ x1+x2+x3,
                      data = prob_03_df,
                      method = 'nnet',
                      metric = my_metric,
                      preProcess = c('center', 'scale'),
                      trControl = my_ctrl,
                      trace = FALSE)

plot(nnet_default)
```

Which tuning parameter combinations are considered to be the best?

According to the figure, the best is when size = 5 decay = 0.1

### 6c)

You will only use the default `caret` tuning grid in this assignment. We could customize it to see if the performance could be improved, but for now the default grid is all we will use.  

**What do the two tuning parameters in the `nnet` package correspond to?**  

#### SOLUTION

What do you think? 

The parameter size represent the number of hidden units and parameter decay means the weight decay